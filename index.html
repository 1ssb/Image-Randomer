<h1>Image Randomiser: A Method to optimize samples using a downsampling and Langevin annealing using JS Divergence.</h1>

<p>This repository contains a Python script that uses Langevin dynamics to process a set of images and save a sample of the optimal images to a destination directory.</p>

<p>PYTHON CODE: <a href="https://github.com/1ssb/Image-Randomiser/blob/main/code.py">https://github.com/1ssb/Image-Randomiser/blob/main/code.py</a></p>

<h2>Overview</h2>

<p>The script loads a set of images from a source directory and downsamples them to speed up computation. It then adds noise to the images during the evaluation process using Langevin dynamics and remembers the names of the optimal images in the sample distribution. After the evaluation process has completed, it copies only those images from the source to the destination directory.</p>

<p>The code uses PyTorch to implement the image processing pipeline and can use multiple GPUs for acceleration if they are available.</p>

<h2>Jensen-Shannon Divergence</h2>

<p>The script uses the Jensen-Shannon (JS) divergence as a measure of similarity between two probability distributions. The JS divergence is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence, which measures how different two probability distributions are.</p>

<p>The motivation behind using the JS divergence is that it is a bounded measure of similarity, meaning that its value always lies between 0 and 1. This makes it easier to interpret and compare the results. Additionally, unlike the KL divergence, the JS divergence is symmetric, meaning that it gives the same result regardless of the order in which the two distributions are compared.</p>

<h2>Langevin Dynamics</h2>

<p>Langevin dynamics is a mathematical framework used to model the movement of particles in a fluid. It is based on the idea of adding random noise to the motion of particles to simulate the effects of thermal fluctuations.</p>

<p>In this script, Langevin dynamics is used to add noise to the images during the evaluation process. This allows the optimization algorithm to explore different regions of the solution space and avoid getting stuck in local minima. The noise is added at multiple levels using an annealing schedule, which gradually reduces the amount of noise as the optimization progresses.</p>

<h2>Usage</h2>

<p>To use the script, you need to specify the source and destination directories and the sample size as arguments to the `process_images` function. The source directory should contain the images you want to process, and the destination directory is where the resulting images will be saved. The sample size determines how many of the optimal images will be saved to the destination directory.</p>

<p>CHECK OUT THE CODE AND MAIL ME IF SOMETHING MESSES UP at <a href="mailto:Subhransu.Bhattacharjee@anu.edu.au">Subhransu.Bhattacharjee@anu.edu.au</a></p>

<h1>References</h1>

<ol>
    <li>Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 681-688).</li>
    <li>Li, C., Zhu, J., & Zhang, B. (2018). Learning energy-based models with exponential family Langevin dynamics. arXiv preprint arXiv:1811.12359.</li>
    <li>Pascanu, R., & Bengio, Y. (2014). Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584.</li>
    <li>Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1), 145-151.</li>
</ol>
